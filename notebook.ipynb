{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185e489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_root: Path = Path(\"/home/krschap/data\")\n",
    "    output_dir: Path = Path(\"outputs\")\n",
    "    seed: int = 64\n",
    "\n",
    "    train_regions: list[str] = field(\n",
    "        default_factory=lambda: ['ramp_dhaka_bangladesh', 'ramp_barishal_bangladesh','ramp_sylhet_bangladesh']\n",
    "    )\n",
    "    val_regions: list[str] = field(\n",
    "        default_factory=lambda: ['ramp_coxs_bazar_bangladesh']\n",
    "    )\n",
    "    test_regions: list[str] = field(\n",
    "        default_factory=lambda: ['stage1_ramp_sample']\n",
    "    )\n",
    "\n",
    "    val_split: float = 0.2\n",
    "    pretrained_model: str = \"facebook/mask2former-swin-base-IN21k-coco-instance\" # https://huggingface.co/facebook/mask2former-swin-base-IN21k-coco-instance\n",
    "\n",
    "    stage1_epochs: int = 10\n",
    "    stage1_batch_size: int = 8\n",
    "    stage1_loader_num_samples: int = 500\n",
    "\n",
    "    # hyper params\n",
    "    stage1_dice_weight: float = 5.0\n",
    "    stage1_mask_weight: float = 5.0\n",
    "    stage1_class_weight: float = 5.0\n",
    "    stage1_learning_rate: float = 0.00001\n",
    "    stage1_weight_decay: float = 0.0001 # penalty on large weights to prevent overfitting\n",
    "    stage1_early_stopping_patience: int = 5\n",
    "\n",
    "    num_workers: int = 32\n",
    "    use_wandb: bool = True\n",
    "    wandb_project: str = \"building-seg-mask2former\"\n",
    "    wandb_run_name: str = \"default_run\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4366dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np \n",
    "import torch \n",
    "\n",
    "def set_seed(seed:int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def get_all_ramp_regions(root: Path) -> list[str]:\n",
    "    regions = [d.name for d in root.iterdir() if d.is_dir() and d.name.startswith(\"ramp_\")]\n",
    "    if not regions:\n",
    "        raise ValueError(f\"No RAMP regions found in {root}\")\n",
    "    return sorted(regions)\n",
    "\n",
    "\n",
    "def split_regions(regions: list[str], val_ratio: float = 0.2, seed: int = 42):\n",
    "    rng = random.Random(seed)\n",
    "    shuffled = regions.copy()\n",
    "    rng.shuffle(shuffled)\n",
    "    split_idx = int(len(shuffled) * (1 - val_ratio))\n",
    "    return shuffled[:split_idx], shuffled[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64161f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "cfg = Config()\n",
    "set_seed(cfg.seed)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3540863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_regions = ['stage1_ramp_sample']\n",
    "cfg.val_regions = ['stage2_sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4844593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_regions = get_all_ramp_regions(cfg.data_root)\n",
    "if cfg.train_regions and cfg.val_regions:\n",
    "    train_regions, val_regions = cfg.train_regions, cfg.val_regions\n",
    "else:\n",
    "    train_regions, val_regions = split_regions(all_regions, cfg.val_split, cfg.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60580b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['stage1_ramp_sample'], ['stage2_sample'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_regions, val_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "458bca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchgeo.datasets import RasterDataset, VectorDataset\n",
    "# from rasterio.crs import CRS\n",
    "from pyproj import CRS\n",
    "\n",
    "class RAMPImageDataset(RasterDataset):\n",
    "    filename_glob = \"*.tif\"\n",
    "    is_image = True\n",
    "    all_bands = (\"R\", \"G\", \"B\")\n",
    "    rgb_bands = (\"R\", \"G\", \"B\")\n",
    "\n",
    "class RAMPMaskDataset(VectorDataset):\n",
    "    filename_glob = \"*.geojson\"\n",
    "\n",
    "    def __init__(self, paths, **kwargs):\n",
    "        super().__init__(paths=paths, task=\"instance_segmentation\", **kwargs)\n",
    "\n",
    "\n",
    "def get_ramp_dataset(root: Path, regions: list[str]):\n",
    "    image_paths , label_paths = [], []\n",
    "\n",
    "    for region in regions:\n",
    "        region_path = root / region\n",
    "        img_path, lbl_path = region_path / \"source\", region_path / \"labels\"\n",
    "        if img_path.exists() and lbl_path.exists():\n",
    "            image_paths.append(img_path)\n",
    "            label_paths.append(lbl_path)\n",
    "\n",
    "    if not image_paths or not label_paths:\n",
    "        raise ValueError(f\"No valid regions found in {root}\")\n",
    "\n",
    "    target_crs = CRS.from_epsg(3857)\n",
    "\n",
    "    print(\"Loading images ...\")\n",
    "    images = RAMPImageDataset(paths=image_paths, crs=target_crs, cache=True, res=0.4)\n",
    "    \n",
    "    print(f\"Loaded {len(images)} image tiles from regions: {regions}\")\n",
    "    print(\"Loading labels ...\")\n",
    "    masks = RAMPMaskDataset(paths=label_paths, crs=target_crs, res=0.4)\n",
    "    print(f\"Loaded {len(masks)} masks tiles from regions: {regions}\")\n",
    "    return images & masks, label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "812f603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images ...\n",
      "Loaded 21 image tiles from regions: ['stage1_ramp_sample']\n",
      "Loading labels ...\n",
      "Loaded 21 masks tiles from regions: ['stage1_ramp_sample']\n",
      "Loading images ...\n",
      "Loaded 8 image tiles from regions: ['stage2_sample']\n",
      "Loading labels ...\n",
      "Loaded 8 masks tiles from regions: ['stage2_sample']\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_label_paths = get_ramp_dataset(cfg.data_root, cfg.train_regions)\n",
    "val_dataset, val_label_paths = get_ramp_dataset(cfg.data_root, cfg.val_regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09df6bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/krschap/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgeo.samplers import RandomGeoSampler\n",
    "import multiprocessing as mp\n",
    "from transformers import Mask2FormerImageProcessor\n",
    "\n",
    "\n",
    "image_processor = Mask2FormerImageProcessor.from_pretrained(\n",
    "    cfg.pretrained_model,\n",
    "    num_labels=2,\n",
    "    do_reduce_labels=True,\n",
    "    ignore_index=255, \n",
    "    size=256,\n",
    "    do_normalize=True,\n",
    ")\n",
    "\n",
    "def collate_fn_mask2former(batch): # source : https://debuggercafe.com/fine-tuning-mask2former/ \n",
    "    \n",
    "    images = [sample['image'].float() for sample in batch]  \n",
    "    inputs = image_processor(images=images, return_tensors=\"pt\")\n",
    "    \n",
    "    \n",
    "    mask_labels = []\n",
    "    class_labels = []\n",
    "\n",
    "    for sample in batch:\n",
    "        mask = sample[\"mask\"]\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        instance_masks = []\n",
    "        instance_classes = []\n",
    "\n",
    "        for i in range(mask.shape[0]):\n",
    "            instance_masks.append(mask[i].float())\n",
    "            instance_classes.append(1)\n",
    "\n",
    "        if instance_masks:\n",
    "            mask_labels.append(torch.stack(instance_masks))\n",
    "            class_labels.append(torch.tensor(instance_classes, dtype=torch.long))\n",
    "        else:\n",
    "            H, W = mask.shape[-2:]\n",
    "            mask_labels.append(torch.zeros((0, H, W), dtype=torch.float32))\n",
    "            class_labels.append(torch.tensor([255], dtype=torch.long))\n",
    "\n",
    "\n",
    "    inputs['mask_labels'] = mask_labels \n",
    "    inputs['class_labels'] = class_labels  \n",
    "    \n",
    "    return inputs\n",
    "    \n",
    "    \n",
    "def create_dataloader(dataset, batch_size, num_samples, num_workers= mp.cpu_count(), is_train=True):\n",
    "    sampler = RandomGeoSampler(dataset, size=256, length=num_samples)\n",
    "    return DataLoader(dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers, pin_memory=True, drop_last=is_train, collate_fn=collate_fn_mask2former)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e0ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader(train_dataset, batch_size=cfg.stage1_batch_size,num_samples=cfg.stage1_loader_num_samples,num_workers=cfg.num_workers,is_train=True)\n",
    "val_loader = create_dataloader(val_dataset, batch_size=cfg.stage1_batch_size,num_samples=cfg.stage1_loader_num_samples,num_workers=cfg.num_workers,is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67ae6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask2FormerConfig {\n",
      "  \"activation_function\": \"relu\",\n",
      "  \"architectures\": [\n",
      "    \"Mask2FormerForUniversalSegmentation\"\n",
      "  ],\n",
      "  \"backbone\": null,\n",
      "  \"backbone_config\": {\n",
      "    \"attention_probs_dropout_prob\": 0.0,\n",
      "    \"depths\": [\n",
      "      2,\n",
      "      2,\n",
      "      18,\n",
      "      2\n",
      "    ],\n",
      "    \"drop_path_rate\": 0.3,\n",
      "    \"embed_dim\": 128,\n",
      "    \"encoder_stride\": 32,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.0,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"image_size\": 224,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"mlp_ratio\": 4.0,\n",
      "    \"model_type\": \"swin\",\n",
      "    \"num_channels\": 3,\n",
      "    \"num_heads\": [\n",
      "      4,\n",
      "      8,\n",
      "      16,\n",
      "      32\n",
      "    ],\n",
      "    \"num_layers\": 4,\n",
      "    \"out_features\": [\n",
      "      \"stage1\",\n",
      "      \"stage2\",\n",
      "      \"stage3\",\n",
      "      \"stage4\"\n",
      "    ],\n",
      "    \"out_indices\": [\n",
      "      1,\n",
      "      2,\n",
      "      3,\n",
      "      4\n",
      "    ],\n",
      "    \"patch_size\": 4,\n",
      "    \"path_norm\": true,\n",
      "    \"qkv_bias\": true,\n",
      "    \"stage_names\": [\n",
      "      \"stem\",\n",
      "      \"stage1\",\n",
      "      \"stage2\",\n",
      "      \"stage3\",\n",
      "      \"stage4\"\n",
      "    ],\n",
      "    \"use_absolute_embeddings\": false,\n",
      "    \"window_size\": 12\n",
      "  },\n",
      "  \"backbone_kwargs\": null,\n",
      "  \"class_weight\": 5.0,\n",
      "  \"common_stride\": 4,\n",
      "  \"decoder_layers\": 10,\n",
      "  \"dice_weight\": 5.0,\n",
      "  \"dim_feedforward\": 2048,\n",
      "  \"dropout\": 0.0,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"encoder_feedforward_dim\": 1024,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"enforce_input_proj\": false,\n",
      "  \"enforce_input_projection\": false,\n",
      "  \"feature_size\": 256,\n",
      "  \"feature_strides\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"hidden_dim\": 256,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"background\",\n",
      "    \"1\": \"building\"\n",
      "  },\n",
      "  \"ignore_index\": 255,\n",
      "  \"ignore_value\": 255,\n",
      "  \"importance_sample_ratio\": 0.75,\n",
      "  \"init_std\": 0.02,\n",
      "  \"init_xavier_std\": 1.0,\n",
      "  \"label2id\": {\n",
      "    \"background\": 0,\n",
      "    \"building\": 1\n",
      "  },\n",
      "  \"mask_feature_size\": 256,\n",
      "  \"mask_weight\": 5.0,\n",
      "  \"model_type\": \"mask2former\",\n",
      "  \"no_object_weight\": 0.1,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 10,\n",
      "  \"num_queries\": 100,\n",
      "  \"output_auxiliary_logits\": null,\n",
      "  \"oversample_ratio\": 3.0,\n",
      "  \"pre_norm\": false,\n",
      "  \"train_num_points\": 12544,\n",
      "  \"transformers_version\": \"4.57.6\",\n",
      "  \"use_auxiliary_loss\": true,\n",
      "  \"use_pretrained_backbone\": false,\n",
      "  \"use_timm_backbone\": false\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Mask2FormerConfig\n",
    "\n",
    "base_config = Mask2FormerConfig.from_pretrained(cfg.pretrained_model)\n",
    "base_config.num_labels = 2\n",
    "base_config.ignore_index = 255\n",
    "base_config.id2label = {0: \"background\", 1: \"building\"}\n",
    "base_config.label2id = {\"background\": 0, \"building\": 1}\n",
    "base_config.class_weight = cfg.stage1_class_weight or 5.0 # default value of this pretrained model is 2.0\n",
    "base_config.dice_weight = cfg.stage1_dice_weight or 5.0# mask quality , 5.0 default\n",
    "base_config.mask_weight = cfg.stage1_mask_weight or 5.0 # mask prediction, 5.0 default\n",
    "print(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7be69c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-coco-instance and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([81]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([81]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Mask2FormerForUniversalSegmentation\n",
    "\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "    cfg.pretrained_model,\n",
    "    config=base_config,\n",
    "    ignore_mismatched_sizes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4e6b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model moved to cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11028e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=cfg.output_dir, \n",
    "    learning_rate=cfg.stage1_learning_rate,\n",
    "    per_device_train_batch_size=cfg.stage1_batch_size,\n",
    "    per_device_eval_batch_size=cfg.stage1_batch_size,\n",
    "    num_train_epochs=cfg.stage1_epochs,\n",
    "    weight_decay=cfg.stage1_weight_decay, \n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  \n",
    "    eval_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False, \n",
    "    dataloader_num_workers=cfg.num_workers, \n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,) # find more here : https://huggingface.co/docs/transformers/v5.0.0rc2/en/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5067b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class TorchGeoTrainer(Trainer):\n",
    "    def __init__(self, *args, train_loader=None, val_loader=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._train_loader = train_loader\n",
    "        self._val_loader = val_loader\n",
    "\n",
    "    def get_train_dataloader(self):\n",
    "        return self._train_loader\n",
    "\n",
    "    def get_eval_dataloader(self, eval_dataset=None):\n",
    "        return self._val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dace168",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchGeoTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[\n",
    "        EarlyStoppingCallback(\n",
    "            early_stopping_patience=cfg.stage1_early_stopping_patience,\n",
    "            early_stopping_threshold=0.01\n",
    "        )\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "664a10d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/krschap/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrschap\u001b[0m (\u001b[33mkrschap-ubs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/krschap/foss/high-res-building-seg-swinv2-mask2former/wandb/run-20260125_232857-2yyvgpy0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/krschap-ubs/huggingface/runs/2yyvgpy0' target=\"_blank\">charmed-terrain-10</a></strong> to <a href='https://wandb.ai/krschap-ubs/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/krschap-ubs/huggingface' target=\"_blank\">https://wandb.ai/krschap-ubs/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/krschap-ubs/huggingface/runs/2yyvgpy0' target=\"_blank\">https://wandb.ai/krschap-ubs/huggingface/runs/2yyvgpy0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='63' max='620' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 63/620 00:13 < 02:04, 4.46 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot sample n_sample > prob_dist.size(-1) samples without replacement",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/trainer.py:2790\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2787\u001b[39m     \u001b[38;5;28mself\u001b[39m.control.should_training_stop = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2789\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_epoch_end(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m-> \u001b[39m\u001b[32m2790\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlearning_rate\u001b[49m\n\u001b[32m   2792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m DebugOption.TPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.debug:\n\u001b[32m   2795\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[32m   2796\u001b[39m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/trainer.py:3221\u001b[39m, in \u001b[36mTrainer._maybe_log_save_evaluate\u001b[39m\u001b[34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[39m\n\u001b[32m   3219\u001b[39m metrics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3220\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.control.should_evaluate:\n\u001b[32m-> \u001b[39m\u001b[32m3221\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3222\u001b[39m     is_new_best_metric = \u001b[38;5;28mself\u001b[39m._determine_best_metric(metrics=metrics, trial=trial)\n\u001b[32m   3224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.save_strategy == SaveStrategy.BEST:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/trainer.py:3170\u001b[39m, in \u001b[36mTrainer._evaluate\u001b[39m\u001b[34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[39m\n\u001b[32m   3169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m3170\u001b[39m     metrics = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3171\u001b[39m     \u001b[38;5;28mself\u001b[39m._report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m.state.global_step, metrics)\n\u001b[32m   3173\u001b[39m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/trainer.py:4489\u001b[39m, in \u001b[36mTrainer.evaluate\u001b[39m\u001b[34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4486\u001b[39m start_time = time.time()\n\u001b[32m   4488\u001b[39m eval_loop = \u001b[38;5;28mself\u001b[39m.prediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.evaluation_loop\n\u001b[32m-> \u001b[39m\u001b[32m4489\u001b[39m output = \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4490\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4492\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[32m   4493\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[32m   4494\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4497\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4499\u001b[39m total_batch_size = \u001b[38;5;28mself\u001b[39m.args.eval_batch_size * \u001b[38;5;28mself\u001b[39m.args.world_size\n\u001b[32m   4500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_jit_compilation_time\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output.metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/transformers/trainer.py:4675\u001b[39m, in \u001b[36mTrainer.evaluation_loop\u001b[39m\u001b[34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[39m\n\u001b[32m   4672\u001b[39m observed_num_examples = \u001b[32m0\u001b[39m\n\u001b[32m   4674\u001b[39m \u001b[38;5;66;03m# Main evaluation loop\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4675\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   4676\u001b[39m     \u001b[38;5;66;03m# Update the observed num examples\u001b[39;00m\n\u001b[32m   4677\u001b[39m     observed_batch_size = find_batch_size(inputs)\n\u001b[32m   4678\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m observed_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:500\u001b[39m, in \u001b[36mDataLoader.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    498\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iterator\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:433\u001b[39m, in \u001b[36mDataLoader._get_iterator\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    432\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_worker_number_rationality()\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1250\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter.__init__\u001b[39m\u001b[34m(self, loader)\u001b[39m\n\u001b[32m   1248\u001b[39m _utils.signal_handling._set_SIGCHLD_handler()\n\u001b[32m   1249\u001b[39m \u001b[38;5;28mself\u001b[39m._worker_pids_set = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1295\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._reset\u001b[39m\u001b[34m(self, loader, first_iter)\u001b[39m\n\u001b[32m   1293\u001b[39m \u001b[38;5;66;03m# prime the prefetch loop\u001b[39;00m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m._prefetch_factor * \u001b[38;5;28mself\u001b[39m._num_workers):\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_put_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1558\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_put_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m   1554\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNumber of outstanding tasks exceeded maximum allowed tasks\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1555\u001b[39m     )\n\u001b[32m   1557\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1558\u001b[39m     index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m   1560\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:731\u001b[39m, in \u001b[36m_BaseDataLoaderIter._next_index\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_index\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m731\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torch/utils/data/sampler.py:341\u001b[39m, in \u001b[36mBatchSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m [*batch_droplast]\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     batch = [*itertools.islice(sampler_iter, \u001b[38;5;28mself\u001b[39m.batch_size)]\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m batch:\n\u001b[32m    343\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foss/high-res-building-seg-swinv2-mask2former/.venv/lib/python3.13/site-packages/torchgeo/samplers/single.py:179\u001b[39m, in \u001b[36mRandomGeoSampler.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the index of a dataset.\u001b[39;00m\n\u001b[32m    173\u001b[39m \n\u001b[32m    174\u001b[39m \u001b[33;03mYields:\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m    [xmin:xmax, ymin:ymax, tmin:tmax] coordinates to index a dataset.\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)):\n\u001b[32m    178\u001b[39m     \u001b[38;5;66;03m# Choose a random tile, weighted by area\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     idx = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mareas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     bounds = \u001b[38;5;28mself\u001b[39m.bounds[idx]\n\u001b[32m    181\u001b[39m     interval = \u001b[38;5;28mself\u001b[39m.intervals[idx]\n",
      "\u001b[31mRuntimeError\u001b[39m: cannot sample n_sample > prob_dist.size(-1) samples without replacement"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914f54d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-res-building-seg-swinv2-mask2former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
