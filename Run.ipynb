{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High res drone imagery with TorchGeo\n",
    "\n",
    "This notebook demonstrates how to use the `OpenAerialMap` dataset in TorchGeo to train a instance segmentation model for building detection.\n",
    "\n",
    "**Features showcased:**\n",
    "1. **Catalog Search:** Querying available imagery using the new `search=True` mode.\n",
    "2. **Data Download:** Downloading specific high-resolution drone imagery.\n",
    "3. **Intersection Dataset:** combining raster (OAM) and vector (OSM) data.\n",
    "\n",
    "\n",
    "The system combines a hierarchical vision transformer backbone with an instance segmentation head to predict individual building footprints from aerial imagery. The architecture is implemented in the `Mask2FormerModule` class and consists of three main components:\n",
    "\n",
    "1. **Swin Transformer V2 Backbone**: Extracts multi-scale features using window-based self-attention\n",
    "2. **Mask2Former Decoder**: Generates instance-level predictions via learnable queries\n",
    "3. **Multi-Component Loss**: Optimizes for classification, overlap, and geometric precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from torchgeo.datasets import OpenStreetMap, OpenAerialMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Region of Interest\n",
    "\n",
    "We define a bounding box for **Banepa Municipality, Nepal**. This area has good drone coverage in OpenAerialMap.\n",
    "\n",
    "![image.png](docs/train_val.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIR = Path('data/banepa')\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "\n",
    "TRAIN_BBOX = [\n",
    "  85.51176609880189,\n",
    "  27.625518932561256,\n",
    "  85.52513148143508,\n",
    "  27.63551883131749\n",
    "]\n",
    "\n",
    "TEST_BBOX = [\n",
    "  85.53039880381334,\n",
    "  27.62456651360527,\n",
    "  85.53606027956683,\n",
    "  27.629042810653335\n",
    "]\n",
    "\n",
    "VAL_BBOX = [\n",
    "  85.51883176039746,\n",
    "  27.63560,\n",
    "  85.52308324197179,\n",
    "  27.63833629629815\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Browse Available Imagery\n",
    "\n",
    "We use `search=True` to query the catalog without downloading files. This allows us to inspect metadata and choose the best image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for available images...\n",
      "Found 4 available images\n",
      "\n",
      "Use .search_results to view.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Provider</th>\n",
       "      <th>GSD</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6708341a4a0ab60001b0b94d</td>\n",
       "      <td>2024-10-10T20:07:54.049+00:00</td>\n",
       "      <td>satellite</td>\n",
       "      <td>Maxar</td>\n",
       "      <td>0.345278</td>\n",
       "      <td>Maxar 105001003E1F0500 Nepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67084ad84a0ab60001b0b95b</td>\n",
       "      <td>2024-10-10T21:44:56.060+00:00</td>\n",
       "      <td>satellite</td>\n",
       "      <td>Maxar</td>\n",
       "      <td>0.345283</td>\n",
       "      <td>Maxar 1040010095519A00 Nepal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62d86c65d8499800053796c4</td>\n",
       "      <td>2022-04-15T19:00:00Z</td>\n",
       "      <td>uav</td>\n",
       "      <td>Geomatics Engineering Society</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>UAV Images of Banepa Municipality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59e62b743d6412ef72209204</td>\n",
       "      <td></td>\n",
       "      <td>satellite</td>\n",
       "      <td>Digital Globe</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>15APR27052125-S3DS_R14C5-054335918020_01_P001.TIF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         ID                           Date   Platform  \\\n",
       "0  6708341a4a0ab60001b0b94d  2024-10-10T20:07:54.049+00:00  satellite   \n",
       "1  67084ad84a0ab60001b0b95b  2024-10-10T21:44:56.060+00:00  satellite   \n",
       "2  62d86c65d8499800053796c4           2022-04-15T19:00:00Z        uav   \n",
       "3  59e62b743d6412ef72209204                                 satellite   \n",
       "\n",
       "                        Provider       GSD  \\\n",
       "0                          Maxar  0.345278   \n",
       "1                          Maxar  0.345283   \n",
       "2  Geomatics Engineering Society  0.031627   \n",
       "3                  Digital Globe  0.400000   \n",
       "\n",
       "                                               Title  \n",
       "0                       Maxar 105001003E1F0500 Nepal  \n",
       "1                       Maxar 1040010095519A00 Nepal  \n",
       "2                  UAV Images of Banepa Municipality  \n",
       "3  15APR27052125-S3DS_R14C5-054335918020_01_P001.TIF  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Searching for available images...')\n",
    "browser = OpenAerialMap(paths=WORK_DIR, bbox=TRAIN_BBOX, search=True, max_items=5)\n",
    "\n",
    "if browser.search_results is not None:\n",
    "    display(browser.search_results.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Datasets\n",
    "\n",
    "We select a specific Image ID from the search results above and download it.\n",
    "We also download OpenStreetMap building footprints for the same area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [TRAIN_BBOX, VAL_BBOX, TEST_BBOX ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenAerialMap (Image Layer)...\n",
      "Using OpenAerialMap image: UAV Images of Banepa Municipality\n",
      "  ID: 62d86c65d8499800053796c4\n",
      "  Collection: openaerialmap\n",
      "  Date: 2022-04-15T19:00:00Z\n",
      "  Platform: uav\n",
      "  Provider: Geomatics Engineering Society\n",
      "  GSD: 0.0316273953754\n",
      "  License: CC-BY-4.0\n",
      "Starting download of 378 tiles...\n",
      "Finished downloading 378 tiles.\n",
      "Download complete.\n",
      "Initializing OpenStreetMap (Mask Layer)...\n",
      "Using OpenAerialMap image: UAV Images of Banepa Municipality\n",
      "  ID: 62d86c65d8499800053796c4\n",
      "  Collection: openaerialmap\n",
      "  Date: 2022-04-15T19:00:00Z\n",
      "  Platform: uav\n",
      "  Provider: Geomatics Engineering Society\n",
      "  GSD: 0.0316273953754\n",
      "  License: CC-BY-4.0\n",
      "Starting download of 40 tiles...\n",
      "Finished downloading 40 tiles.\n",
      "Download complete.\n",
      "Initializing OpenStreetMap (Mask Layer)...\n",
      "Using OpenAerialMap image: UAV Images of Banepa Municipality\n",
      "  ID: 62d86c65d8499800053796c4\n",
      "  Collection: openaerialmap\n",
      "  Date: 2022-04-15T19:00:00Z\n",
      "  Platform: uav\n",
      "  Provider: Geomatics Engineering Society\n",
      "  GSD: 0.0316273953754\n",
      "  License: CC-BY-4.0\n",
      "Starting download of 72 tiles...\n",
      "Finished downloading 72 tiles.\n",
      "Download complete.\n",
      "Initializing OpenStreetMap (Mask Layer)...\n"
     ]
    }
   ],
   "source": [
    "# Selected Image ID (from search results above)\n",
    "IMAGE_ID = '62d86c65d8499800053796c4'\n",
    "ZOOM_LEVEL = 19\n",
    "CHIP_SIZE_PX = 256\n",
    "\n",
    "print('Initializing OpenAerialMap (Image Layer)...')\n",
    "\n",
    "\n",
    "folder = ['train', 'val', 'test']\n",
    "\n",
    "i = 0\n",
    "for bbox_d in all_datasets:\n",
    "    oam_dataset = OpenAerialMap(\n",
    "        paths=os.path.join(WORK_DIR,folder[i],'source'),\n",
    "        bbox=bbox_d,\n",
    "        zoom=ZOOM_LEVEL,\n",
    "        download=True,\n",
    "        image_id=IMAGE_ID,\n",
    "        tile_size=CHIP_SIZE_PX,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print('Initializing OpenStreetMap (Mask Layer)...')\n",
    "    OSM_CLASSES = [{'name': 'building', 'selector': [{'building': '*'}]}]\n",
    "\n",
    "    osm_dataset = OpenStreetMap(\n",
    "        paths=os.path.join(WORK_DIR, folder[i], 'labels'), bbox=bbox_d, classes=OSM_CLASSES, download=True\n",
    "    )\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Intersection Dataset & Sampler\n",
    "\n",
    "We use the `&` operator to create an IntersectionDataset. This ensures every sample contains both imagery and a corresponding mask. this is done inside the package , lets initialize the configuration with dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(data_root=PosixPath('data/banepa'), output_dir='outputs/banepa_experiment', seed=64, train_regions=['train'], val_regions=['val'], test_regions=['test'], val_split=0.2, pretrained_model='facebook/mask2former-swin-base-IN21k-coco-instance', epochs=50, batch_size=8, dice_weight=5.0, mask_weight=5.0, class_weight=5.0, boundary_loss_weight=5.0, compactness_loss_weight=5.0, learning_rate=1e-05, weight_decay=0.0001, early_stopping_patience=10, num_workers=31, use_wandb=True, wandb_project='building-seg-mask2former', wandb_run_name='notebook_run_banepa')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/krschap/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkrschap\u001b[0m (\u001b[33mkrschap-ubs\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "from src.config import Config\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "cfg.data_root = WORK_DIR\n",
    "cfg.output_dir = 'outputs/banepa_experiment'\n",
    "cfg.train_regions = ['train']\n",
    "cfg.val_regions = ['val']\n",
    "cfg.test_regions = ['test']\n",
    "cfg.wandb_run_name='notebook_run_banepa'\n",
    "cfg.use_wandb = True \n",
    "\n",
    "\n",
    "cfg.epochs = 50\n",
    "\n",
    "print(cfg)\n",
    "\n",
    "if cfg.use_wandb:\n",
    "    import wandb \n",
    "    wandb.login()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Setup\n",
    "\n",
    "The `Mask2FormerModule` class wraps the pretrained `Mask2FormerForUniversalSegmentation` model from Hugging Face Transformers. Model configuration occurs in the `__init__` method:\n",
    "\n",
    "| Configuration Parameter | Value | Purpose |\n",
    "|------------------------|-------|---------|\n",
    "| `pretrained_model` | `facebook/mask2former-swin-base-IN21k-coco-instance` | Base model with ImageNet-22K pretrained backbone |\n",
    "| `num_labels` | 2 | Binary classification: background (0), building (1) |\n",
    "| `ignore_index` | 255 | Label value to ignore in loss computation (empty masks) |\n",
    "| `class_weight` | 5.0 (default) | Weight for classification loss component |\n",
    "| `dice_weight` | 5.0 (default) | Weight for Dice loss component |\n",
    "| `mask_weight` | 5.0 (default) | Weight for mask loss component |\n",
    "\n",
    "The model is initialized from pretrained weights with `ignore_mismatched_sizes=True` to accommodate the change from COCO's 80 classes to binary classification.\n",
    "\n",
    "![arch](docs/transformer_arc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/transformers/image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-base-IN21k-coco-instance and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([3, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([81]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([81]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from src.utils import set_seed\n",
    "from src.stage1_foundation import Mask2FormerModule\n",
    "from src.stage1_foundation import OAMDataModule\n",
    "set_seed(cfg.seed)\n",
    "\n",
    "model = Mask2FormerModule(cfg)\n",
    "datamodule = OAMDataModule(cfg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses the AdamW optimizer with cosine annealing learning rate schedule, configured in the `configure_optimizers()` method:\n",
    "\n",
    "| Parameter | Default Value | Description |\n",
    "|-----------|---------------|-------------|\n",
    "| Optimizer | AdamW | Weight decay variant of Adam |\n",
    "| Learning Rate | 1e-5 | Initial learning rate from `cfg.learning_rate` |\n",
    "| Weight Decay | 1e-4 | L2 regularization penalty from `cfg.weight_decay` |\n",
    "| Scheduler | CosineAnnealingLR | Gradually reduces LR to 0 over training |\n",
    "| T_max | `cfg.epochs` | Number of epochs for full cosine cycle |\n",
    "\n",
    "The cosine annealing schedule provides:\n",
    "- Smooth learning rate decay from initial value to near-zero\n",
    "- Better convergence properties than step decay\n",
    "- No need for manual LR adjustment during training\n",
    "\n",
    "The scheduler updates once per epoch (`interval=\"epoch\"`, `frequency=1`) with strict enforcement of the schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/run-20260203_235646-iyf9rdla</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/krschap-ubs/building-seg-mask2former/runs/iyf9rdla' target=\"_blank\">notebook_run_banepa</a></strong> to <a href='https://wandb.ai/krschap-ubs/building-seg-mask2former' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/krschap-ubs/building-seg-mask2former' target=\"_blank\">https://wandb.ai/krschap-ubs/building-seg-mask2former</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/krschap-ubs/building-seg-mask2former/runs/iyf9rdla' target=\"_blank\">https://wandb.ai/krschap-ubs/building-seg-mask2former/runs/iyf9rdla</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding image,label path for ['train']...\n",
      "Loading images ...\n",
      "Loaded 378 image tiles. using crs : GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]] with res (2.682209014892578e-06, 2.3762066170690543e-06)\n",
      "Loading labels ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:881: Checkpoint directory /home/krschap/code/foss/swinv2-mask2former-drone/outputs/banepa_experiment exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name          | Type                                | Params | Mode  | FLOPs\n",
      "--------------------------------------------------------------------------------------\n",
      "0 | model         | Mask2FormerForUniversalSegmentation | 106 M  | eval  | 0    \n",
      "1 | train_metrics | MetricCollection                    | 0      | train | 0    \n",
      "2 | val_metrics   | MetricCollection                    | 0      | train | 0    \n",
      "3 | test_metrics  | MetricCollection                    | 0      | train | 0    \n",
      "4 | train_map     | MeanAveragePrecision                | 0      | train | 0    \n",
      "5 | val_map       | MeanAveragePrecision                | 0      | train | 0    \n",
      "6 | test_map      | MeanAveragePrecision                | 0      | train | 0    \n",
      "--------------------------------------------------------------------------------------\n",
      "106 M     Trainable params\n",
      "0         Non-trainable params\n",
      "106 M     Total params\n",
      "427.537   Total estimated model params size (MB)\n",
      "21        Modules in train mode\n",
      "719       Modules in eval mode\n",
      "0         Total Flops\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 mask tiles. using crs : EPSG:4326 with res (0.0001, 0.0001)\n",
      "Converting RAMPMaskDataset res from (0.0001, 0.0001) to (2.682209014892578e-06, 2.3762066170690543e-06)\n",
      "Train dataset length: 378\n",
      "Finding image,label path for ['val']...\n",
      "Loading images ...\n",
      "Loaded 40 image tiles. using crs : GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]] with res (2.682209014892578e-06, 2.376153779362622e-06)\n",
      "Loading labels ...\n",
      "Loaded 1 mask tiles. using crs : EPSG:4326 with res (0.0001, 0.0001)\n",
      "Converting RAMPMaskDataset res from (0.0001, 0.0001) to (2.682209014892578e-06, 2.376153779362622e-06)\n",
      "Val dataset length: 40\n",
      "Finding image,label path for ['test']...\n",
      "Loading images ...\n",
      "Loaded 72 image tiles. using crs : GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]] with res (2.682209014892578e-06, 2.3763519047398374e-06)\n",
      "Loading labels ...\n",
      "Loaded 1 mask tiles. using crs : EPSG:4326 with res (0.0001, 0.0001)\n",
      "Converting RAMPMaskDataset res from (0.0001, 0.0001) to (2.682209014892578e-06, 2.3763519047398374e-06)\n",
      "Test dataset length: 72\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]val_sampler length 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/pytorch_lightning/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/pytorch_lightning/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_sampler length 608                                                   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/pytorch_lightning/utilities/_pytree.py:21: `isinstance(treespec, LeafSpec)` is deprecated, use `isinstance(treespec, TreeSpec) and treespec.is_leaf()` instead.\n",
      "/home/krschap/code/foss/swinv2-mask2former-drone/.venv/lib/python3.13/site-packages/pytorch_lightning/loops/fit_loop.py:534: Found 719 module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is intentional, you can ignore this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:  75%|███████▌  | 57/76 [00:40<00:13,  1.40it/s, v_num=rdla, train_loss=33.20, val_acc=0.922, val_f1=0.859, val_iou=0.753, val_p=0.809, val_r=0.916, val_map=0.337, val_map_50=0.633]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=cfg.early_stopping_patience,\n",
    "        mode=\"min\",\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=cfg.output_dir,\n",
    "        filename=\"best\",\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "    ),\n",
    "]\n",
    "\n",
    "logger = (\n",
    "    WandbLogger(project=cfg.wandb_project, name=cfg.wandb_run_name)\n",
    "    if cfg.use_wandb\n",
    "    else None\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=cfg.epochs,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    precision=\"16-mixed\",\n",
    "    default_root_dir=cfg.output_dir,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test & Visualize Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(model, datamodule, ckpt_path=\"best\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "**Instance-Level (AP/mAP)** - Evaluates each building separately:\n",
    "- `map` - mAP @ IoU 0.50:0.95 (strict, best for boundaries)\n",
    "- `map_50` - mAP @ IoU 0.50 (detection quality)\n",
    "\n",
    "**Pixel-Level (Binary)** - Overall coverage:\n",
    "- `iou` - Intersection over Union\n",
    "- `f1` - F1 score\n",
    "- `acc` - Accuracy\n",
    "\n",
    "For building boundaries & separation, prioritize **`map`** and **`map_50`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule.setup()\n",
    "batch = next(iter(datamodule.test_dataloader()))\n",
    "model.visualize_batch(batch, num_samples=5, save_path=\"docs/test_results.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "high-res-building-seg-swinv2-mask2former",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
